{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import h5py\n",
    "#import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "#% matplotlib inline\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import InputSpec, Input, Dense, Flatten, Permute#, Layer\n",
    "from keras.layers import Lambda, Masking, Activation, Dropout, Embedding, TimeDistributed\n",
    "from keras.layers import Bidirectional, GRU, LSTM\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.merge import Add, Dot, add, concatenate, dot\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.engine.topology import Layer\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def create_base_network(input_dim, hidden_dim, dropout=0.0):\n",
    "    '''Base network to be shared (eq. to feature extraction).'''\n",
    "    seq = Sequential()\n",
    "    seq.add(Dense(hidden_dim, input_shape=(input_dim,), activation='relu'))\n",
    "    seq.add(Dropout(dropout))\n",
    "    seq.add(Dense(hidden_dim, activation='relu'))\n",
    "    seq.add(Dropout(dropout))\n",
    "    seq.add(Dense(hidden_dim, activation='relu'))\n",
    "    return seq\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(input_dim, hidden_dim, settings):    \n",
    "    # network definition\n",
    "    base_network = create_base_network(input_dim, hidden_dim, settings['dropout'])\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    \n",
    "    # because we re-use the same instance `base_network`,\n",
    "    # the weights of the network\n",
    "    # will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "    \n",
    "    model = Model(inputs=[input_a, input_b], outputs=distance)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_base_network(vectors, shape, settings):\n",
    "    max_length, nr_hidden, nr_class = shape\n",
    "    \n",
    "    # Inputs\n",
    "    #ids1 = Input(shape=(max_length,), dtype='int32', name='words1')\n",
    "    #ids2 = Input(shape=(max_length,), dtype='int32', name='words2')\n",
    "    \n",
    "    seq = Sequential()\n",
    "    \n",
    "    seq.add(Embedding(vectors.shape[0],\n",
    "                      vectors.shape[1],\n",
    "                      input_length=max_length,\n",
    "                      weights=[vectors],\n",
    "                      name='embed',\n",
    "                      trainable=False))\n",
    "    \n",
    "    seq.add(LSTM(nr_hidden, dropout=settings['dropout']))\n",
    "    \n",
    "    seq.add(Dense(nr_hidden, activation='relu', dropout=settings['dropout']))\n",
    "    \n",
    "    return(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(input_dim, shape, settings):\n",
    "    # network definition\n",
    "    base_network = create_base_network(input_dim, shape, settings['dropout'])\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    \n",
    "    # because we re-use the same instance `base_network`,\n",
    "    # the weights of the network\n",
    "    # will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "    \n",
    "    model = Model(inputs=[input_a, input_b], outputs=distance)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    125461\n",
       "1     74472\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/input/train.csv', index_col='id')\n",
    "\n",
    "train_df['question1'].fillna('', inplace=True)\n",
    "train_df['question2'].fillna('', inplace=True)\n",
    "\n",
    "# filter out question shorter than 10 characters\n",
    "train_df['q1_len'] = train_df['question1'].str.len()\n",
    "train_df['q2_len'] = train_df['question2'].str.len()\n",
    "\n",
    "train_df = train_df.loc[lambda df: (df['q1_len'] > 10) & (df['q2_len'] > 10)]\n",
    "\n",
    "# subset data\n",
    "train_df = train_df.loc[0:199999]\n",
    "\n",
    "train_df['is_duplicate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "print('Splitting data set')\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(train_df['question1'].values, \n",
    "                                                                      train_df['question2'].values,\n",
    "                                                                      train_df['is_duplicate'].values, \n",
    "                                                                      test_size=0.2, random_state=42, \n",
    "                                                                      stratify=train_df['is_duplicate'].values)\n",
    "\n",
    "print('Get doc vectors')\n",
    "print('Train 1')\n",
    "train_X1 = get_word_ids(list(nlp.pipe(X1_train.tolist(), n_threads=20, batch_size=10000)),\n",
    "                        max_length=30,\n",
    "                        rnn_encode=False,\n",
    "                        tree_truncate=False)\n",
    "print('Train 2')\n",
    "train_X2 = get_word_ids(list(nlp.pipe(X2_train.tolist(), n_threads=20, batch_size=10000)),\n",
    "                        max_length=30,\n",
    "                        rnn_encode=False,\n",
    "                        tree_truncate=False)\n",
    "print('Val 1')\n",
    "val_X1 = get_word_ids(list(nlp.pipe(X1_val.tolist(), n_threads=20, batch_size=10000)),\n",
    "                        max_length=30,\n",
    "                        rnn_encode=False,\n",
    "                        tree_truncate=False)\n",
    "print('Val 2')\n",
    "val_X2 = get_word_ids(list(nlp.pipe(X2_val.tolist(), n_threads=20, batch_size=10000)),\n",
    "                        max_length=30,\n",
    "                        rnn_encode=False,\n",
    "                        tree_truncate=False)\n",
    "\n",
    "print('Building model')\n",
    "shape = (30, 200, 1)\n",
    "settings = {\n",
    "    'lr': 0.0001,\n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "model = create_network(300, shape, settings)\n",
    "model.compile(\n",
    "        optimizer=Adam(lr=settings['lr']),\n",
    "        loss='binary_crossentropy',\n",
    "        #loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data set\n",
      "Get doc vectors\n",
      "Train 1\n",
      "Train 2\n",
      "Val 1\n",
      "Val 2\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "print('Splitting data set')\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(train_df['question1'].values, \n",
    "                                                                      train_df['question2'].values,\n",
    "                                                                      train_df['is_duplicate'].values, \n",
    "                                                                      test_size=0.2, random_state=42, \n",
    "                                                                      stratify=train_df['is_duplicate'].values)\n",
    "\n",
    "print('Get doc vectors')\n",
    "print('Train 1')\n",
    "train_X1 = np.array([doc.vector for doc in nlp.pipe(X1_train.tolist(), n_threads=20, batch_size=10000)])\n",
    "print('Train 2')\n",
    "train_X2 = np.array([doc.vector for doc in nlp.pipe(X2_train.tolist(), n_threads=20, batch_size=10000)])\n",
    "print('Val 1')\n",
    "val_X1 = np.array([doc.vector for doc in nlp.pipe(X1_val.tolist(), n_threads=20, batch_size=10000)])\n",
    "print('Val 2')\n",
    "val_X2 = np.array([doc.vector for doc in nlp.pipe(X2_val.tolist(), n_threads=20, batch_size=10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model\n"
     ]
    }
   ],
   "source": [
    "print('Building model')\n",
    "shape = (30, 200, 1)\n",
    "settings = {\n",
    "    'lr': 0.0001,\n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "model = create_network(300, 128, settings)\n",
    "model.compile(\n",
    "        optimizer=Adam(lr=settings['lr']),\n",
    "        loss='binary_crossentropy',\n",
    "        #loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 300)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)        (None, 128)           71552       input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 1)             0           sequential_1[1][0]               \n",
      "                                                                   sequential_1[2][0]               \n",
      "====================================================================================================\n",
      "Total params: 71,552\n",
      "Trainable params: 71,552\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159946 samples, validate on 39987 samples\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_3/kernel:0 is illegal; using dense_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_3/bias:0 is illegal; using dense_3/bias_0 instead.\n",
      "Epoch 1/100\n",
      "5s - loss: 0.7447 - acc: 0.5389 - val_loss: 0.9449 - val_acc: 0.6275\n",
      "Epoch 2/100\n",
      "5s - loss: 0.6548 - acc: 0.6292 - val_loss: 0.9143 - val_acc: 0.6274\n",
      "Epoch 3/100\n",
      "5s - loss: 0.6355 - acc: 0.6463 - val_loss: 0.8985 - val_acc: 0.6274\n",
      "Epoch 4/100\n",
      "5s - loss: 0.6238 - acc: 0.6578 - val_loss: 0.9086 - val_acc: 0.6276\n",
      "Epoch 5/100\n",
      "5s - loss: 0.6160 - acc: 0.6670 - val_loss: 0.9261 - val_acc: 0.6279\n",
      "Epoch 6/100\n",
      "5s - loss: 0.6103 - acc: 0.6725 - val_loss: 0.9197 - val_acc: 0.6284\n",
      "Epoch 7/100\n",
      "5s - loss: 0.6086 - acc: 0.6740 - val_loss: 0.9344 - val_acc: 0.6281\n",
      "Epoch 8/100\n",
      "5s - loss: 0.6049 - acc: 0.6772 - val_loss: 0.9468 - val_acc: 0.6286\n",
      "Epoch 9/100\n",
      "5s - loss: 0.6021 - acc: 0.6800 - val_loss: 0.9481 - val_acc: 0.6287\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb97ff84ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5, verbose=1),\n",
    "    keras.callbacks.CSVLogger(filename='..\\logs\\checkpoints\\model_log.csv'),\n",
    "    keras.callbacks.ModelCheckpoint('..\\models\\checkpoints\\weights.{epoch:03d}-{val_loss:0.4f}.hdf5',\n",
    "                                    save_best_only=True,\n",
    "                                    save_weights_only=True,\n",
    "                                    period=10),\n",
    "    keras.callbacks.TensorBoard(log_dir='..\\logs\\\\tensorboard',\n",
    "                                histogram_freq=1)\n",
    "]\n",
    "\n",
    "# Train model\n",
    "model.fit([train_X1, train_X2],\n",
    "          y_train,\n",
    "          validation_data=([val_X1, val_X2], y_val),\n",
    "          epochs=100,\n",
    "          batch_size=256, \n",
    "          verbose=2,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save('../models/model_siamese_network.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('../models/model_siamese_network.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict([val_X1, val_X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39987"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pred > 1)\n",
    "len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
